[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://MinfengZhu.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["minfengzhu"],"categories":null,"content":"Minfeng Zhu is a Ph.D student at State Key Lab of CAD\u0026amp;CG, Zhejiang University, supervised by Prof. Wei Chen.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d2e2d2887bb13006210f344c3d2b4856","permalink":"https://MinfengZhu.github.io/authors/minfengzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minfengzhu/","section":"authors","summary":"Minfeng Zhu is a Ph.D student at State Key Lab of CAD\u0026amp;CG, Zhejiang University, supervised by Prof. Wei Chen.","tags":null,"title":"Minfeng Zhu","type":"authors"},{"authors":["Minfeng Zhu","Pingbo Pan","Wei Chen","and Yi Yang."],"categories":[],"content":"","date":1585813562,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585813562,"objectID":"085a1a8b4fcc9c17df6936522de234f7","permalink":"https://MinfengZhu.github.io/publication/eemefn/","publishdate":"2020-04-02T15:46:02+08:00","relpermalink":"/publication/eemefn/","section":"publication","summary":"This work focuses on the extremely low-light image enhancement, which aims to improve image brightness and reveal hidden information in darken areas. Recently, image enhancement approaches have yielded impressive progress. However, existing methods still suffer from three main problems: (1) low-light images usually are high-contrast. Existing methods may fail to recover images details in extremely dark or bright areas; (2) current methods cannot precisely correct the color of low-light images; (3) when the object edges are unclear, the pixel-wise loss may treat pixels of different objects equally and produce blurry images. In this paper, we propose a two-stage method called Edge-Enhanced Multi-Exposure Fusion Network (EEMEFN) to enhance extremely low-light images. In the first stage, we employ a multi-exposure fusion module to address the high contrast and color bias issues. We synthesize a set of images with different exposure time from a single image and construct an accurate normal-light image by combining well-exposed areas under different illumination conditions. Thus, it can produce realistic initial images with correct color from extremely noisy and low-light images. Secondly, we introduce an edge enhancement module to refine the initial images with the help of the edge information. Therefore, our method can reconstruct high-quality images with sharp edges when minimizing the pixel-wise loss. Experiments on the See-in-the-Dark dataset indicate that our EEMEFN approach achieves state-of-the-art performance.","tags":["Low-light Image Enhancement"],"title":"EEMEFN: Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network","type":"publication"},{"authors":["Minfeng Zhu","Pingbo Pan","Wei Chen","and Yi Yang."],"categories":[],"content":"","date":1554191162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554191162,"objectID":"70239dc94e06475a7978e5b238d8e84f","permalink":"https://MinfengZhu.github.io/publication/dm-gan/","publishdate":"2019-04-02T15:46:02+08:00","relpermalink":"/publication/dm-gan/","section":"publication","summary":"In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.","tags":["Generative Adversarial Networks","Text-to-Image Synthesis"],"title":"DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis","type":"publication"},{"authors":["Minfeng Zhu","Wei Chen","Jiazhi Xia","Yuxin Ma","Yankong Zhang","Yuetong Luo","Zhaosong Huang","Liangjun Liu"],"categories":[],"content":"","date":1552296004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552296004,"objectID":"6de916936e6e1d6592d35c0d0ceb4176","permalink":"https://MinfengZhu.github.io/publication/location2vec/","publishdate":"2019-03-11T17:20:04+08:00","relpermalink":"/publication/location2vec/","section":"publication","summary":"Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.","tags":["Human mobility","word embedding","urban computing","spatio-temporal data","visual exploration"],"title":"location2vec: a situation-aware representation for visual exploration of urban locations","type":"publication"},{"authors":["Wei Chen","Zhaosong Huang","Feiran Wu","Minfeng Zhu","Huihua Guan","and Ross Maciejewski."],"categories":[],"content":"","date":1541144762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541144762,"objectID":"816bde2fec75bdb8e2d20358198738dd","permalink":"https://MinfengZhu.github.io/publication/vaud/","publishdate":"2018-11-02T15:46:02+08:00","relpermalink":"/publication/vaud/","section":"publication","summary":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.","tags":["Urban data","Visual Analysis","Visual Reasoning","Heterogeneous","Spatio-temporal"],"title":"VAUD: A visual analysis approach for exploring spatio-temporal urban data","type":"publication"},{"authors":["Tianlong Gu","Minfeng Zhu","Wei Chen","Zhaosong Huang","Ross Maciejewski","and Liang Chang."],"categories":[],"content":"","date":1533195962,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533195962,"objectID":"74f997771a1ca6d75fc235b2958d0447","permalink":"https://MinfengZhu.github.io/publication/amtg/","publishdate":"2018-08-02T15:46:02+08:00","relpermalink":"/publication/amtg/","section":"publication","summary":"Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.","tags":["Timeline","Mobility","Mobility Transition","Mobility Patterns"],"title":"Structuring Mobility Transition With an Adaptive Graph Representation","type":"publication"}]