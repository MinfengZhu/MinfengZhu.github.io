<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: November 21, 2022 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css" />

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  


























  
  
  






  <meta name="author" content="Minfeng Zhu 朱闽峰" />





  

<meta name="description" content="Minfeng Zhu" />



<link rel="alternate" hreflang="en-us" href="https://minfengzhu.github.io/publication/" />
<link rel="canonical" href="https://minfengzhu.github.io/publication/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hubcc6da888a29c8354be6ce60e7e29f30_403445_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hubcc6da888a29c8354be6ce60e7e29f30_403445_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://minfengzhu.github.io/media/icon_hubcc6da888a29c8354be6ce60e7e29f30_403445_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="Minfeng Zhu" />
<meta property="og:url" content="https://minfengzhu.github.io/publication/" />
<meta property="og:title" content="Publications | Minfeng Zhu" />
<meta property="og:description" content="Minfeng Zhu" /><meta property="og:image" content="https://minfengzhu.github.io/media/icon_hubcc6da888a29c8354be6ce60e7e29f30_403445_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta property="og:updated_time" content="2022-04-07T00:00:00&#43;00:00" />
  










  
  
  

  
  
    <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Minfeng Zhu" />
  

  


  
  <title>Publications | Minfeng Zhu</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  




  <div class="page-header header--fixed">
    












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Minfeng Zhu</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Minfeng Zhu</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    
















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Bo Wang</span>, <span >
      Tao Wu</span>, <span >
      Minfeng Zhu</span>, <span >
      Peng Du</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April, 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      CVPR
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/plgan/" >Interactive Image Synthesis with Panoptic Layout Generation</a>
  </div>

  
  <a href="/publication/plgan/"  class="summary-link">
    <div class="article-style">
      <p>The PLGAN employs panoptic theory which distinguishes object categories between&quot; stuff&quot; with amorphous boundaries and&quot; things&quot; with well-defined shapes, such that stuff and instance layouts are constructed through separate branches and later fused into panoptic layouts.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Interactive_Image_Synthesis_With_Panoptic_Layout_Generation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/plgan/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/wb-finalking/PLGAN" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Haozhe Feng</span>, <span >
      Zhaoyang You</span>, <span >
      Minghao Chen</span>, <span >
      Tianye Zhang</span>, <span >
      Minfeng Zhu</span>, <span >
      Fei Wu</span>, <span >
      Chao Wu</span>, <span >
      Wei Chen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      ICML
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kd3a/" >location2vec: a situation-aware representation for visual exploration of urban locations</a>
  </div>

  
  <a href="/publication/kd3a/"  class="summary-link">
    <div class="article-style">
      <p>Conventional unsupervised multi-source domain adaptation (UMDA) methods assume all source domains can be accessed directly. This neglects the privacy-preserving policy, that is, all the data and computations must be kept decentralized. There exists three problems in this scenario: (1) Minimizing the domain distance requires the pairwise calculation of the data from source and target domains, which is not accessible. (2) The communication cost and privacy security limit the application of UMDA methods (e.g., the domain adversarial training). (3) Since users have no authority to check the data quality, the irrelevant or malicious source domains are more likely to appear, which causes negative transfer. In this study, we propose a privacy-preserving UMDA paradigm named Knowledge Distillation based Decentralized Domain Adaptation (KD3A), which performs domain adaptation through the knowledge distillation on models from different source domains. KD3A solves the above problems with three components: (1) A multi-source knowledge distillation method named Knowledge Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting strategy named Consensus Focus to identify both the malicious and irrelevant domains. (3) A decentralized optimization strategy for domain distance named BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is robust to the negative transfer and brings a 100x reduction of communication cost compared with other decentralized UMDA methods. Moreover, our KD3A significantly outperforms state-of-the-art UMDA approaches.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2011.09757.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kd3a/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Jiacheng Pan</span>, <span >
      Wei Chen</span>, <span >
      Xiaodong Zhao</span>, <span >
      Minfeng Zhu</span>, <span >
      Jian Chen</span>, <span >
      Siwei Fu</span>, <span >
      Yingcai Wu</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TVCG
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/examplerlayout/" >Exemplar-based Layout Fine-tuning for Node-link Diagrams</a>
  </div>

  
  <a href="/publication/examplerlayout/"  class="summary-link">
    <div class="article-style">
      <p>We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.cad.zju.edu.cn/home/vagblog/images/photo_bed/2020/8/19/7d531afa561ac4fa5febffe0cb95e38477f73241.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/examplerlayout/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Minfeng Zhu</span>, <span >
      Wei Chen</span>, <span >
      Yuanzhe Hu</span>, <span >
      Yuxuan Hou</span>, <span >
      Liangjun Liu</span>, <span >
      Kaiyuan Zhang</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TVCG
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/drgraph/" >DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction</a>
  </div>

  
  <a href="/publication/drgraph/"  class="summary-link">
    <div class="article-style">
      <p>Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/9282195" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/drgraph/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Zhaosong Huang</span>, <span >
      Ye Zhao</span>, <span >
      Wei Chen</span>, <span >
      Shengjie Gao</span>, <span >
      Kejie Yu</span>, <span >
      Weixia Xu</span>, <span >
      Mingjie Tang</span>, <span >
      Minfeng Zhu</span>, <span >
      Mingliang Xu</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November, 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TVCG
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/nl/" >A Natural-language-based Visual Query Approach of Uncertain Human Trajectories</a>
  </div>

  
  <a href="/publication/nl/"  class="summary-link">
    <div class="article-style">
      <p>Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/8807274/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/nl/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Minfeng Zhu</span>, <span >
      Pingbo Pan</span>, <span >
      Wei Chen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April, 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      AAAI
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/eemefn/" >EEMEFN: Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network</a>
  </div>

  
  <a href="/publication/eemefn/"  class="summary-link">
    <div class="article-style">
      <p>This work focuses on the extremely low-light image enhancement, which aims to improve image brightness and reveal hidden information in darken areas. Recently, image enhancement approaches have yielded impressive progress. However, existing methods still suffer from three main problems: (1) low-light images usually are high-contrast. Existing methods may fail to recover images details in extremely dark or bright areas; (2) current methods cannot precisely correct the color of low-light images; (3) when the object edges are unclear, the pixel-wise loss may treat pixels of different objects equally and produce blurry images. In this paper, we propose a two-stage method called Edge-Enhanced Multi-Exposure Fusion Network (EEMEFN) to enhance extremely low-light images. In the first stage, we employ a multi-exposure fusion module to address the high contrast and color bias issues. We synthesize a set of images with different exposure time from a single image and construct an accurate normal-light image by combining well-exposed areas under different illumination conditions. Thus, it can produce realistic initial images with correct color from extremely noisy and low-light images. Secondly, we introduce an edge enhancement module to refine the initial images with the help of the edge information. Therefore, our method can reconstruct high-quality images with sharp edges when minimizing the pixel-wise loss. Experiments on the See-in-the-Dark dataset indicate that our EEMEFN approach achieves state-of-the-art performance.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhuM.2559.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/eemefn/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Minfeng Zhu</span>, <span >
      Pingbo Pan</span>, <span >
      Wei Chen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April, 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      CVPR
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dm-gan/" >DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis</a>
  </div>

  
  <a href="/publication/dm-gan/"  class="summary-link">
    <div class="article-style">
      <p>In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1904.01310" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dm-gan/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Minfeng Zhu</span>, <span >
      Wei Chen</span>, <span >
      Jiazhi Xia</span>, <span >
      Yuxin Ma</span>, <span >
      Yankong Zhang</span>, <span >
      Yuetong Luo</span>, <span >
      Zhaosong Huang</span>, <span >
      Liangjun Liu</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March, 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TITS
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/location2vec/" >location2vec: a situation-aware representation for visual exploration of urban locations</a>
  </div>

  
  <a href="/publication/location2vec/"  class="summary-link">
    <div class="article-style">
      <p>Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.cad.zju.edu.cn/home/vagblog/VAG_Work/Location2Vec_ITS_FINAL.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/location2vec/cite.bib">
  Cite
</a>















  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Wei Chen</span>, <span >
      Zhaosong Huang</span>, <span >
      Feiran Wu</span>, <span >
      Minfeng Zhu</span>, <span >
      Huihua Guan</span>, <span >
      Ross Maciejewski</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November, 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TVCG
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vaud/" >VAUD: A visual analysis approach for exploring spatio-temporal urban data</a>
  </div>

  
  <a href="/publication/vaud/"  class="summary-link">
    <div class="article-style">
      <p>Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.cad.zju.edu.cn/home/vagblog/VAG_Work/VAUD.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vaud/cite.bib">
  Cite
</a>











  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://zhaosongh.github.io/publication/VAUD.mp4" target="_blank" rel="noopener">
  Video
</a>





  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Tianlong Gu</span>, <span >
      Minfeng Zhu</span>, <span >
      Wei Chen</span>, <span >
      Zhaosong Huang</span>, <span >
      Ross Maciejewski</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August, 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE TCSS
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/amtg/" >Structuring Mobility Transition With an Adaptive Graph Representation</a>
  </div>

  
  <a href="/publication/amtg/"  class="summary-link">
    <div class="article-style">
      <p>Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.cad.zju.edu.cn/home/vagblog/VAG_Work/AMTG.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/amtg/cite.bib">
  Cite
</a>











  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.cad.zju.edu.cn/home/vagblog/VAG_Work/AMTG.mp4" target="_blank" rel="noopener">
  Video
</a>





  </div>
  

</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>




  
    <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
  

  
  

  






























<script id="page-data" type="application/json">{"use_headroom":true}</script>



  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.e973d49cb2d568aa6f8eaf3638337473.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>


















</body>
</html>
